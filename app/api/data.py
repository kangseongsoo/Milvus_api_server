"""
ë°ì´í„° ê´€ë¦¬ API (CRUD)
"""
from fastapi import APIRouter, HTTPException, status, Query
from app.models.document import (
    DocumentInsertRequest,
    DocumentInsertResponse,
    BatchInsertRequest,
    BatchInsertResponse,
    BatchInsertWithEmbeddingsRequest,
    DocumentResponse,
    DocumentUpdateRequest,
    DocumentUpdateResponse,
    DocumentDeleteRequest,
    DocumentDeleteResponse,
    BotDeleteRequest,
    BotDeleteResponse,
    MetadataUpdateRequest,
    MetadataUpdateResponse,
    DuplicateCheckRequest,
    DuplicateCheckResponse
)
from app.schemas.milvus_metadata import filter_milvus_metadata
from app.utils.logger import setup_logger
from app.core.auto_flusher import auto_flusher
from app.core.postgres_client import postgres_client
from app.core.milvus_client import milvus_client
from app.core.embedding import embedding_service
from app.config import settings
from pymilvus import Collection
from datetime import datetime

logger = setup_logger(__name__)
router = APIRouter()


def generate_partition_name(chat_bot_id: str) -> str:
    """chat_bot_idë¡œ íŒŒí‹°ì…˜ëª… ìë™ ìƒì„±"""
    return f"bot_{chat_bot_id.replace('-', '')}"


@router.post("/check-duplicates", response_model=DuplicateCheckResponse, status_code=status.HTTP_200_OK)
async def check_duplicates(request: DuplicateCheckRequest):
    """
    ì¤‘ë³µ ê²€ì‚¬ API
    
    data/insert ë˜ëŠ” data/insert/batch í˜¸ì¶œ ì „ì— ì¤‘ë³µì„ í™•ì¸í•©ë‹ˆë‹¤.
    account_name, chat_bot_id, content_name ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ
    ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë¬¸ì„œ(ì¤‘ë³µ)ì™€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œ(ì¤‘ë³µ ì•ˆë¨)ë¥¼ êµ¬ë¶„í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    
    ì¤‘ë³µ ê¸°ì¤€: chat_bot_id + content_name ì¡°í•©
    """
    try:
        start_time = datetime.now()
        
        logger.info(f"ğŸ” ì¤‘ë³µ ê²€ì‚¬ ì‹œì‘")
        logger.info(f"   - Account: {request.account_name}")
        logger.info(f"   - Bot ID: {request.chat_bot_id}")
        logger.info(f"   - ìš”ì²­í•œ content_names: {len(request.content_name)}ê°œ")
        
        # PostgreSQLì—ì„œ ì¡´ì¬í•˜ëŠ” content_nameë“¤ ì¡°íšŒ
        existing_content_names = await postgres_client.get_existing_content_names(
            account_name=request.account_name,
            chat_bot_id=request.chat_bot_id,
            content_names=request.content_name
        )
        
        # ì¤‘ë³µëœ ê²ƒê³¼ ì¤‘ë³µë˜ì§€ ì•Šì€ ê²ƒ êµ¬ë¶„
        existing_set = set(existing_content_names)
        requested_set = set(request.content_name)
        
        duplicate_content_names = list(existing_set)
        unique_content_names = list(requested_set - existing_set)
        
        total_time = (datetime.now() - start_time).total_seconds() * 1000
        
        logger.info(f"âœ… ì¤‘ë³µ ê²€ì‚¬ ì™„ë£Œ")
        logger.info(f"   - ìš”ì²­ëœ ë¬¸ì„œ: {len(request.content_name)}ê°œ")
        logger.info(f"   - ì¤‘ë³µëœ ë¬¸ì„œ: {len(duplicate_content_names)}ê°œ")
        logger.info(f"   - ì¤‘ë³µë˜ì§€ ì•Šì€ ë¬¸ì„œ: {len(unique_content_names)}ê°œ")
        logger.info(f"   - ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ms")
        
        return DuplicateCheckResponse(
            status="success",
            total_requested=len(request.content_name),
            duplicate_count=len(duplicate_content_names),
            unique_count=len(unique_content_names),
            duplicate_content_names=duplicate_content_names,
            unique_content_names=unique_content_names
        )
        
    except Exception as e:
        logger.error(f"âŒ ì¤‘ë³µ ê²€ì‚¬ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to check duplicates: {str(e)}"
        )


@router.post("/insert", response_model=DocumentInsertResponse, status_code=status.HTTP_201_CREATED)
async def insert_document(request: DocumentInsertRequest):
    """
    ë¬¸ì„œ ë°ì´í„° ì‚½ì… (ê°œì„ ëœ Saga Pattern)
    
    ì „ì²˜ë¦¬ ì„œë²„ë¡œë¶€í„° ê³„ì •ëª… + ë©”íƒ€ë°ì´í„° + í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ë°›ì•„
    1. PostgreSQL íŠ¸ëœì­ì…˜ìœ¼ë¡œ ë¬¸ì„œ+ì²­í¬ ì›ìì„± ë³´ì¥
    2. ì„ë² ë”© ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    3. Milvus ë²¡í„° ì €ì¥ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    4. ì‹¤íŒ¨ ì‹œ ë³´ìƒ íŠ¸ëœì­ì…˜ìœ¼ë¡œ PostgreSQL ë¡¤ë°±
    """
    doc_id = None
    try:
        start_time = datetime.now()
        collection_name = f"collection_{request.account_name}"
        partition_name = generate_partition_name(request.chat_bot_id)
        
        logger.info(f"ğŸ“ ë¬¸ì„œ ì‚½ì… ì‹œì‘ (Saga Pattern)")
        logger.info(f"   - Account: {request.account_name}")
        logger.info(f"   - Bot ID: {request.chat_bot_id}")
        logger.info(f"   - Title: {request.metadata.get('title', '(ì œëª© ì—†ìŒ)')}")
        logger.info(f"   - Chunks: {len(request.chunks)}")
        logger.info(f"   - Collection: {collection_name}")
        logger.info(f"   - Partition: {partition_name}")
        
        # ========== Step 0: ì¤‘ë³µ ì²´í¬ ==========
        existing_content_names = await postgres_client.get_existing_content_names(
            account_name=request.account_name,
            chat_bot_id=request.chat_bot_id,
            content_names=[request.content_name]
        )
        
        if existing_content_names:
            # ì¤‘ë³µëœ ë¬¸ì„œê°€ ì¡´ì¬í•¨
            logger.warning(f"âš ï¸ ì¤‘ë³µëœ ë¬¸ì„œ ë°œê²¬, ìŠ¤í‚µ: content_name='{request.content_name}'")
            total_time = (datetime.now() - start_time).total_seconds() * 1000
            
            return DocumentInsertResponse(
                status="skipped",
                doc_id=None,
                total_chunks=len(request.chunks),
                skipped=True,
                error_message=f"ë¬¸ì„œê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {request.content_name}",
                postgres_insert_time_ms=0.0,
                embedding_time_ms=0.0,
                milvus_insert_time_ms=0.0,
                total_time_ms=total_time
            )
        
        # ========== Step 1: ë©”íƒ€ë°ì´í„° ë¶„ë¦¬ ==========
        all_metadata = request.metadata or {}
        milvus_metadata = filter_milvus_metadata(all_metadata)  # Milvus í•„í„°ë§ìš©ë§Œ ì¶”ì¶œ
        
        logger.info(f"ğŸ“Š ë©”íƒ€ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ:")
        logger.info(f"   - Milvus í•„í„°ë§ìš©: {len(milvus_metadata)}ê°œ í•„ë“œ")
        logger.info(f"   - PostgreSQL ì „ì²´: {len(all_metadata)}ê°œ í•„ë“œ (ì „ì²´ ì €ì¥)")
        
        # ========== Step 2: PostgreSQL íŠ¸ëœì­ì…˜ (ì›ìì„± ë³´ì¥) ==========
        postgres_start = datetime.now()
        
        doc_id = await postgres_client.insert_document_with_chunks_transaction(
            account_name=request.account_name,
            document_data={
                "chat_bot_id": request.chat_bot_id,
                "content_name": request.content_name,  # ë¬¸ì„œ ê³ ìœ  ì‹ë³„ì
                "metadata": all_metadata  # ì „ì²´ ë©”íƒ€ë°ì´í„°ë¥¼ PostgreSQLì— ì €ì¥
            },
            chunks=[{"chunk_index": c.chunk_index, "text": c.text, "content_hash": c.content_hash} for c in request.chunks]
        )
        
        # ì¤‘ë³µ ë¬¸ì„œ ì²˜ë¦¬: doc_idê°€ Noneì´ë©´ ì¤‘ë³µì´ë¯€ë¡œ ìŠ¤í‚µ
        if doc_id is None:
            logger.warning(f"âš ï¸ ì¤‘ë³µëœ ë¬¸ì„œ ë°œê²¬ (PostgreSQL ë°˜í™˜): content_name='{request.content_name}', ìŠ¤í‚µ")
            total_time = (datetime.now() - start_time).total_seconds() * 1000
            
            return DocumentInsertResponse(
                status="skipped",
                doc_id=None,
                total_chunks=len(request.chunks),
                skipped=True,
                error_message=f"ë¬¸ì„œê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {request.content_name} (ë™ì‹œ ì‚½ì… ì‹œë„)",
                postgres_insert_time_ms=0.0,
                embedding_time_ms=0.0,
                milvus_insert_time_ms=0.0,
                total_time_ms=total_time
            )
        
        postgres_time = (datetime.now() - postgres_start).total_seconds() * 1000
        logger.info(f"âœ… PostgreSQL íŠ¸ëœì­ì…˜ ì™„ë£Œ: doc_id={doc_id}")
        
        # ========== Step 2: ì„ë² ë”© ìƒì„± (ì¬ì‹œë„ ë¡œì§) ==========
        embedding_start = datetime.now()
        
        try:
            embeddings = await embedding_service.batch_embed_with_retry(
                texts=[chunk.text for chunk in request.chunks],
                max_retries=3,
                backoff=2.0
            )
            embedding_time = (datetime.now() - embedding_start).total_seconds() * 1000
            logger.info(f"âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ: {len(embeddings)}ê°œ ë²¡í„°")
            
        except Exception as embedding_error:
            # ì„ë² ë”© ì‹¤íŒ¨ ì‹œ PostgreSQL ë¡¤ë°±
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨, PostgreSQL ë¡¤ë°± ì‹œì‘: {str(embedding_error)}")
            await postgres_client.delete_document(request.account_name, request.chat_bot_id, doc_id)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(embedding_error)}"
            )
        
        # ========== Step 3: Milvus ë²¡í„° ì €ì¥ (ì¬ì‹œë„ ë¡œì§) ==========
        milvus_start = datetime.now()
        
        try:
            await milvus_client.insert_vectors_with_retry(
                account_name=request.account_name,
                chat_bot_id=request.chat_bot_id,
                partition_name=partition_name,
                doc_id=doc_id,
                content_name=request.content_name,  # content_name ì¶”ê°€
                chunks=[
                    {
                        "chunk_index": chunk.chunk_index,
                        "embedding": embeddings[idx],
                        "text": chunk.text
                    }
                    for idx, chunk in enumerate(request.chunks)
                ],
                metadata=milvus_metadata,  # í•„í„°ë§ìš© ë©”íƒ€ë°ì´í„°ë§Œ Milvusì—
                max_retries=3,
                backoff=2.0
            )
            milvus_time = (datetime.now() - milvus_start).total_seconds() * 1000
            logger.info(f"âœ… Milvus ë²¡í„° ì‚½ì… ì™„ë£Œ")
            
        except Exception as milvus_error:
            # Milvus ì‹¤íŒ¨ ì‹œ PostgreSQL ë¡¤ë°± (ë³´ìƒ íŠ¸ëœì­ì…˜)
            logger.error(f"âŒ Milvus ì‚½ì… ì‹¤íŒ¨, PostgreSQL ë¡¤ë°± ì‹œì‘: {str(milvus_error)}")
            await postgres_client.delete_document(request.account_name, request.chat_bot_id, doc_id)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Milvus ì‚½ì… ì‹¤íŒ¨: {str(milvus_error)}"
            )
        
        # ========== Step 4: ğŸ”¥ ìë™ flush ë§ˆí‚¹ (ì´ë²¤íŠ¸ ê¸°ë°˜) ==========
        await auto_flusher.mark_for_flush(collection_name)
        logger.info(f"ğŸ”¥ Flush marked: {collection_name} (will flush within 0.5s)")
        
        total_time = (datetime.now() - start_time).total_seconds() * 1000
        
        logger.info(f"âœ… ë¬¸ì„œ ì‚½ì… ì™„ë£Œ (Saga Pattern ì„±ê³µ)")
        logger.info(f"   - doc_id: {doc_id}")
        logger.info(f"   - ì²­í¬ ìˆ˜: {len(request.chunks)}")
        logger.info(f"   - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ms")
        logger.info(f"   - ê²€ìƒ‰ ê°€ëŠ¥ ì‹œê°„: 0.5ì´ˆ ì´ë‚´")
        
        return DocumentInsertResponse(
            status="success",
            doc_id=doc_id,
            total_chunks=len(request.chunks),
            postgres_insert_time_ms=postgres_time,
            embedding_time_ms=embedding_time,
            milvus_insert_time_ms=milvus_time,
            total_time_ms=total_time
        )
        
    except HTTPException:
        # HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì¬ë°œìƒ
        raise
    except Exception as e:
        # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ì‹œ PostgreSQL ë¡¤ë°±
        if doc_id is not None:
            logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ, PostgreSQL ë¡¤ë°± ì‹œì‘: {str(e)}")
            try:
                await postgres_client.delete_document(request.account_name, request.chat_bot_id, doc_id)
                logger.info(f"âœ… PostgreSQL ë¡¤ë°± ì™„ë£Œ: doc_id={doc_id}")
            except Exception as rollback_error:
                logger.error(f"âŒ PostgreSQL ë¡¤ë°± ì‹¤íŒ¨: {str(rollback_error)}")
        
        logger.error(f"âŒ ë¬¸ì„œ ì‚½ì… ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to insert document: {str(e)}"
        )


@router.post("/insert/batch", response_model=BatchInsertResponse, status_code=status.HTTP_201_CREATED)
async def batch_insert_documents(request: BatchInsertRequest):
    """
    ì—¬ëŸ¬ ë¬¸ì„œ ì¼ê´„ ì‚½ì… (ê°œì„ ëœ Saga Pattern)
    
    ì „ì²˜ë¦¬ ì„œë²„ì—ì„œ ì—¬ëŸ¬ ë¬¸ì„œë¥¼ í•œ ë²ˆì— ì „ì†¡
    1. PostgreSQL íŠ¸ëœì­ì…˜ìœ¼ë¡œ ëª¨ë“  ë¬¸ì„œ+ì²­í¬ ì›ìì„± ë³´ì¥
    2. ë°°ì¹˜ ì„ë² ë”© ìƒì„± (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    3. Milvus ë°°ì¹˜ ë²¡í„° ì €ì¥ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    4. ì‹¤íŒ¨ ì‹œ ë³´ìƒ íŠ¸ëœì­ì…˜ìœ¼ë¡œ PostgreSQL ë¡¤ë°±
    """
    doc_ids = []
    try:
        start_time = datetime.now()
        total_docs = len(request.documents)
        collection_name = f"collection_{request.account_name}"
        
        logger.info(f"ğŸ“ ë°°ì¹˜ ì‚½ì… ì‹œì‘ (Saga Pattern)")
        logger.info(f"   - Account: {request.account_name}")
        logger.info(f"   - Documents: {total_docs}ê°œ")
        logger.info(f"   - Collection: {collection_name}")
        
        # ========== Step 0: ì¤‘ë³µ ì²´í¬ ==========
        # ë™ì¼í•œ chat_bot_idë¥¼ ê°€ì§„ ë¬¸ì„œë“¤ë§Œ ì²˜ë¦¬ (í˜¼í•©ëœ ë´‡ ID í—ˆìš©)
        content_names = [doc.content_name for doc in request.documents]
        
        # ê°™ì€ ë´‡ì˜ ë¬¸ì„œë“¤ì„ ê·¸ë£¹í™”
        bot_id_map = {}
        for i, doc in enumerate(request.documents):
            if doc.chat_bot_id not in bot_id_map:
                bot_id_map[doc.chat_bot_id] = []
            bot_id_map[doc.chat_bot_id].append((i, doc))
        
        # ê° ë´‡ë³„ë¡œ ì¤‘ë³µ ì²´í¬
        unique_docs = []  # ì¤‘ë³µë˜ì§€ ì•Šì€ ë¬¸ì„œë“¤ì˜ ì›ë³¸ ì¸ë±ìŠ¤ì™€ ë¬¸ì„œ ì •ë³´
        failed_docs = []  # ì¤‘ë³µëœ ë¬¸ì„œë“¤ì˜ ì •ë³´
        
        for bot_id, doc_list in bot_id_map.items():
            bot_content_names = [doc.content_name for _, doc in doc_list]
            existing_names = await postgres_client.get_existing_content_names(
                account_name=request.account_name,
                chat_bot_id=bot_id,
                content_names=bot_content_names
            )
            
            existing_set = set(existing_names)
            for idx, doc in doc_list:
                if doc.content_name in existing_set:
                    # ì¤‘ë³µëœ ë¬¸ì„œ
                    failed_docs.append({
                        "content_name": doc.content_name,
                        "title": doc.metadata.get('title', '(ì œëª© ì—†ìŒ)') if doc.metadata else '(ì œëª© ì—†ìŒ)',
                        "reason": "ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë¬¸ì„œ"
                    })
                    logger.warning(f"âš ï¸ ì¤‘ë³µëœ ë¬¸ì„œ ë°œê²¬, ìŠ¤í‚µ: content_name='{doc.content_name}'")
                else:
                    # ì¤‘ë³µë˜ì§€ ì•Šì€ ë¬¸ì„œ
                    unique_docs.append((idx, doc))
        
        # ëª¨ë“  ë¬¸ì„œê°€ ì¤‘ë³µì¸ ê²½ìš°
        if not unique_docs:
            total_time = (datetime.now() - start_time).total_seconds() * 1000
            logger.warning(f"âš ï¸ ëª¨ë“  ë¬¸ì„œê°€ ì¤‘ë³µë¨, ìŠ¤í‚µ: {total_docs}ê°œ")
            
            from app.models.document import BatchInsertResult
            return BatchInsertResponse(
                status="skipped",
                total_documents=total_docs,
                total_chunks=sum(len(doc.chunks) for doc in request.documents),
                success_count=0,
                failure_count=len(failed_docs),
                inserted_documents=0,
                total_vectors=0,
                failed_content_names=[doc["content_name"] for doc in failed_docs],
                results=[BatchInsertResult(
                    doc_id=0,
                    title=doc["title"],
                    total_chunks=0,
                    success=False,
                    error=doc["reason"]
                ) for doc in failed_docs],
                postgres_insert_time_ms=0.0,
                embedding_time_ms=0.0,
                milvus_insert_time_ms=0.0,
                total_time_ms=total_time
            )
        
        # ì¤‘ë³µë˜ì§€ ì•Šì€ ë¬¸ì„œë“¤ë¡œë§Œ ì¬êµ¬ì„±
        logger.info(f"ğŸ“‹ ì¤‘ë³µ ì²´í¬ ì™„ë£Œ: {len(unique_docs)}ê°œ ì‚½ì…, {len(failed_docs)}ê°œ ì¤‘ë³µ ìŠ¤í‚µ")
        unique_doc_list = [doc for _, doc in unique_docs]
        total_chunks = sum(len(doc.chunks) for doc in unique_doc_list)
        
        # ========== Step 1~3: ë¬¸ì„œë³„ ê°œë³„ ì²˜ë¦¬ (ë¶€ë¶„ ì‹¤íŒ¨ í—ˆìš©) ==========
        postgres_start = datetime.now()
        embedding_start = datetime.now()
        milvus_start = datetime.now()
        
        successful_docs = []  # ì„±ê³µí•œ ë¬¸ì„œë“¤ì˜ ì •ë³´ (doc, doc_id)
        failed_processing_docs = []  # ì²˜ë¦¬ ì¤‘ ì‹¤íŒ¨í•œ ë¬¸ì„œë“¤
        
        for i, doc in enumerate(unique_doc_list):
            doc_id = None
            try:
                # ========== Step 1: PostgreSQL ì‚½ì… ==========
                all_metadata = doc.metadata or {}
                doc_id = await postgres_client.insert_document_with_chunks_transaction(
                    account_name=request.account_name,
                    document_data={
                        "chat_bot_id": doc.chat_bot_id,
                        "content_name": doc.content_name,
                        "metadata": all_metadata
                    },
                    chunks=[{"chunk_index": c.chunk_index, "text": c.text, "content_hash": c.content_hash} for c in doc.chunks]
                )
                
                # ì¤‘ë³µ ë¬¸ì„œ ì²˜ë¦¬: doc_idê°€ Noneì´ë©´ ì¤‘ë³µì´ë¯€ë¡œ ìŠ¤í‚µ
                if doc_id is None:
                    logger.warning(f"âš ï¸ ì¤‘ë³µëœ ë¬¸ì„œ ë°œê²¬ (PostgreSQL ë°˜í™˜): content_name='{doc.content_name}', ìŠ¤í‚µ")
                    failed_processing_docs.append({
                        "content_name": doc.content_name,
                        "title": doc.metadata.get('title', '(ì œëª© ì—†ìŒ)') if doc.metadata else '(ì œëª© ì—†ìŒ)',
                        "reason": "ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë¬¸ì„œ (ë™ì‹œ ì‚½ì… ì‹œë„)"
                    })
                    continue
                
                # ========== Step 2: ì„ë² ë”© ìƒì„± ==========
                embeddings = await embedding_service.batch_embed_with_retry(
                    texts=[chunk.text for chunk in doc.chunks],
                    max_retries=3,
                    backoff=2.0
                )
                
                # ========== Step 3: Milvus ì‚½ì… ==========
                partition_name = generate_partition_name(doc.chat_bot_id)
                milvus_metadata = filter_milvus_metadata(all_metadata)
                
                chunks_with_embeddings = []
                for idx, chunk in enumerate(doc.chunks):
                    chunks_with_embeddings.append({
                        "chunk_index": chunk.chunk_index,
                        "embedding": embeddings[idx],
                        "text": chunk.text
                    })
                
                await milvus_client.insert_vectors_with_retry(
                    account_name=request.account_name,
                    chat_bot_id=doc.chat_bot_id,
                    partition_name=partition_name,
                    doc_id=doc_id,
                    content_name=doc.content_name,
                    chunks=chunks_with_embeddings,
                    metadata=milvus_metadata,
                    max_retries=3,
                    backoff=2.0
                )
                
                # ëª¨ë“  ë‹¨ê³„ ì„±ê³µ
                successful_docs.append((doc, doc_id))
                logger.info(f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: content_name='{doc.content_name}', doc_id={doc_id}")
                
            except Exception as e:
                # ì¤‘ë³µ ì˜¤ë¥˜ ì˜ˆì™¸ ì²˜ë¦¬ (ì•ˆì „ì¥ì¹˜)
                error_str = str(e)
                if "duplicate key value violates unique constraint" in error_str or "unique constraint" in error_str.lower():
                    logger.warning(f"âš ï¸ ì¤‘ë³µëœ ë¬¸ì„œ ë°œê²¬ (ì˜ˆì™¸ ì²˜ë¦¬): content_name='{doc.content_name}', ìŠ¤í‚µ")
                    failed_processing_docs.append({
                        "content_name": doc.content_name,
                        "title": doc.metadata.get('title', '(ì œëª© ì—†ìŒ)') if doc.metadata else '(ì œëª© ì—†ìŒ)',
                        "reason": "ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë¬¸ì„œ (ë™ì‹œ ì‚½ì… ì‹œë„)"
                    })
                    continue
                
                # ì–´ëŠ ë‹¨ê³„ë“  ì‹¤íŒ¨í•˜ë©´ í•´ë‹¹ ë¬¸ì„œë§Œ ë¡¤ë°±
                logger.error(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨: content_name='{doc.content_name}', ì˜¤ë¥˜: {str(e)}")
                
                if doc_id is not None:
                    # PostgreSQLì— ì‚½ì…ëœ ê²½ìš° ë¡¤ë°±
                    try:
                        await postgres_client.delete_document(request.account_name, doc.chat_bot_id, doc_id)
                        logger.info(f"âœ… ë¡¤ë°± ì™„ë£Œ: doc_id={doc_id}")
                    except Exception as rollback_error:
                        logger.error(f"âŒ ë¡¤ë°± ì‹¤íŒ¨: doc_id={doc_id}, ì˜¤ë¥˜: {str(rollback_error)}")
                
                # ì‹¤íŒ¨ ì •ë³´ ì €ì¥
                failed_processing_docs.append({
                    "content_name": doc.content_name,
                    "title": doc.metadata.get('title', '(ì œëª© ì—†ìŒ)') if doc.metadata else '(ì œëª© ì—†ìŒ)',
                    "reason": str(e)
                })
        
        # ì„±ê³µí•œ ë¬¸ì„œë“¤ì˜ doc_id ì¶”ì¶œ
        doc_ids = [doc_id for _, doc_id in successful_docs]
        
        # ì‹œê°„ ì¸¡ì •
        postgres_time = (datetime.now() - postgres_start).total_seconds() * 1000
        embedding_time = (datetime.now() - embedding_start).total_seconds() * 1000
        milvus_time = (datetime.now() - milvus_start).total_seconds() * 1000
        
        logger.info(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: ì„±ê³µ {len(successful_docs)}ê°œ, ì‹¤íŒ¨ {len(failed_processing_docs)}ê°œ (ì²˜ë¦¬ ì¤‘)")
        
        # ========== Step 4: ğŸ”¥ ìë™ flush ë§ˆí‚¹ (ì´ë²¤íŠ¸ ê¸°ë°˜) ==========
        await auto_flusher.mark_for_flush(collection_name)
        logger.info(f"ğŸ”¥ Flush marked: {collection_name}")
        
        total_time = (datetime.now() - start_time).total_seconds() * 1000
        
        # ì„±ê³µ ë° ì‹¤íŒ¨ ê²°ê³¼ ìƒì„±
        from app.models.document import BatchInsertResult
        results = []
        
        # ì„±ê³µí•œ ë¬¸ì„œë“¤ ì¶”ê°€
        for doc, doc_id in successful_docs:
            results.append(BatchInsertResult(
                doc_id=doc_id,
                title=doc.metadata.get('title', '(ì œëª© ì—†ìŒ)') if doc.metadata else '(ì œëª© ì—†ìŒ)',
                total_chunks=len(doc.chunks),
                success=True,
                error=None
            ))
        
        # ì¤‘ë³µìœ¼ë¡œ ìŠ¤í‚µëœ ë¬¸ì„œë“¤ ì¶”ê°€
        for failed_doc in failed_docs:
            results.append(BatchInsertResult(
                doc_id=0,
                title=failed_doc["title"],
                total_chunks=0,
                success=False,
                error=failed_doc["reason"]
            ))
        
        # ì²˜ë¦¬ ì¤‘ ì‹¤íŒ¨í•œ ë¬¸ì„œë“¤ ì¶”ê°€
        for failed_doc in failed_processing_docs:
            results.append(BatchInsertResult(
                doc_id=0,
                title=failed_doc["title"],
                total_chunks=0,
                success=False,
                error=failed_doc["reason"]
            ))
        
        # ëª¨ë“  ì‹¤íŒ¨ ë¬¸ì„œ í•©ì¹˜ê¸°
        all_failed_docs = failed_docs + failed_processing_docs
        total_failed = len(all_failed_docs)
        
        # status ê²°ì •
        if total_failed == 0:
            response_status = "success"
        elif len(successful_docs) == 0:
            response_status = "skipped"
        else:
            response_status = "partial_success"
        
        # ì´ ì‚½ì…ëœ ì²­í¬ ìˆ˜ ê³„ì‚° (ì„±ê³µí•œ ë¬¸ì„œë“¤ë§Œ)
        total_inserted_chunks = sum(len(doc.chunks) for doc, _ in successful_docs)
        
        logger.info(f"âœ… ë°°ì¹˜ ì‚½ì… ì™„ë£Œ")
        logger.info(f"   - ìš”ì²­: {total_docs}ê°œ ë¬¸ì„œ")
        logger.info(f"   - ì„±ê³µ: {len(successful_docs)}ê°œ ë¬¸ì„œ")
        logger.info(f"   - ì‹¤íŒ¨: {total_failed}ê°œ ë¬¸ì„œ (ì¤‘ë³µ: {len(failed_docs)}, ì²˜ë¦¬ì¤‘: {len(failed_processing_docs)})")
        logger.info(f"   - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ms")
        logger.info(f"   - ê²€ìƒ‰ ê°€ëŠ¥ ì‹œê°„: 0.5ì´ˆ ì´ë‚´")
        
        return BatchInsertResponse(
            status=response_status,
            total_documents=total_docs,
            total_chunks=sum(len(doc.chunks) for doc in request.documents),  # ì „ì²´ ìš”ì²­ ì²­í¬ ìˆ˜
            success_count=len(successful_docs),
            failure_count=total_failed,
            inserted_documents=len(successful_docs),
            total_vectors=total_inserted_chunks,  # ì„±ê³µí•œ ë¬¸ì„œë“¤ì˜ ì²­í¬ ìˆ˜
            failed_content_names=[doc["content_name"] for doc in all_failed_docs],
            results=results,
            postgres_insert_time_ms=postgres_time,
            embedding_time_ms=embedding_time,
            milvus_insert_time_ms=milvus_time,
            total_time_ms=total_time
        )
        
    except HTTPException:
        # HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì¬ë°œìƒ
        raise
    except Exception as e:
        # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ì‹¤íŒ¨ ì‘ë‹µ ë°˜í™˜
        # ë¬¸ì„œë³„ ì²˜ë¦¬ê°€ë¯€ë¡œ ì´ë¯¸ ë¡¤ë°± ì²˜ë¦¬ë¨
        logger.error(f"âŒ ë°°ì¹˜ ì‚½ì… ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Batch insert failed: {str(e)}"
        )


@router.post("/insert/batch/with-embeddings", response_model=BatchInsertResponse, status_code=status.HTTP_201_CREATED)
async def batch_insert_documents_with_embeddings(request: BatchInsertWithEmbeddingsRequest):
    """
    ì„ë² ë”©ì„ í¬í•¨í•œ ì—¬ëŸ¬ ë¬¸ì„œ ì¼ê´„ ì‚½ì… (ë§ˆì´ê·¸ë ˆì´ì…˜ìš©)
    
    ê¸°ì¡´ PostgreSQLì— ì €ì¥ëœ ì„ë² ë”© ë²¡í„°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ Milvusì— ì‚½ì…
    - ì„ë² ë”© ìƒì„± ë‹¨ê³„ë¥¼ ìŠ¤í‚µí•˜ê³  ì œê³µëœ ë²¡í„°ë¥¼ ì§ì ‘ ì‚¬ìš©
    - PostgreSQL íŠ¸ëœì­ì…˜ìœ¼ë¡œ ëª¨ë“  ë¬¸ì„œ+ì²­í¬ ì›ìì„± ë³´ì¥
    - Milvus ë°°ì¹˜ ë²¡í„° ì €ì¥ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    - ì‹¤íŒ¨ ì‹œ ë³´ìƒ íŠ¸ëœì­ì…˜ìœ¼ë¡œ PostgreSQL ë¡¤ë°±
    """
    doc_ids = []
    try:
        start_time = datetime.now()
        total_docs = len(request.documents)
        total_chunks = sum(len(doc.chunks) for doc in request.documents)
        collection_name = f"collection_{request.account_name}"
        
        logger.info(f"ğŸ“ ë°°ì¹˜ ì‚½ì… ì‹œì‘ (ì„ë² ë”© í¬í•¨, ë§ˆì´ê·¸ë ˆì´ì…˜ìš©)")
        logger.info(f"   - Account: {request.account_name}")
        logger.info(f"   - Documents: {total_docs}ê°œ")
        logger.info(f"   - Total Chunks: {total_chunks}ê°œ")
        logger.info(f"   - Collection: {collection_name}")
        logger.info(f"   - ì„ë² ë”© ìƒì„±: ìŠ¤í‚µ (ê¸°ì¡´ ë²¡í„° ì‚¬ìš©)")
        
        # ========== Step 1: PostgreSQL ë°°ì¹˜ íŠ¸ëœì­ì…˜ (ì›ìì„± ë³´ì¥) ==========
        postgres_start = datetime.now()
        
        # ë¬¸ì„œ ë°ì´í„° ì¤€ë¹„ (ë©”íƒ€ë°ì´í„° ë¶„ë¦¬)
        documents_data = []
        for doc in request.documents:
            # ë©”íƒ€ë°ì´í„° ë¶„ë¦¬
            all_metadata = doc.metadata or {}
            
            documents_data.append({
                "document_data": {
                    "chat_bot_id": doc.chat_bot_id,
                    "content_name": doc.content_name,  # ë¬¸ì„œ ê³ ìœ  ì‹ë³„ì
                    "metadata": all_metadata  # ì „ì²´ ë©”íƒ€ë°ì´í„°ë¥¼ PostgreSQLì— ì €ì¥
                },
                "chunks": [{"chunk_index": c.chunk_index, "text": c.text, "content_hash": c.content_hash} for c in doc.chunks]
            })
        
        doc_ids = await postgres_client.batch_insert_documents_with_chunks_transaction(
            account_name=request.account_name,
            documents=documents_data
        )
        
        postgres_time = (datetime.now() - postgres_start).total_seconds() * 1000
        logger.info(f"âœ… PostgreSQL ë°°ì¹˜ íŠ¸ëœì­ì…˜ ì™„ë£Œ: {len(doc_ids)}ê°œ ë¬¸ì„œ")
        
        # ========== Step 2: ì„ë² ë”© ìƒì„± ìŠ¤í‚µ (ì´ë¯¸ ì œê³µë¨) ==========
        embedding_start = datetime.now()
        embedding_time = 0.0  # ì„ë² ë”© ìƒì„±í•˜ì§€ ì•ŠìŒ
        logger.info(f"â­ï¸ ì„ë² ë”© ìƒì„± ìŠ¤í‚µ (ê¸°ì¡´ ë²¡í„° ì‚¬ìš©): {total_chunks}ê°œ ë²¡í„°")
        
        # ========== Step 3: Milvus ë°°ì¹˜ ë²¡í„° ì €ì¥ (ì¬ì‹œë„ ë¡œì§) ==========
        milvus_start = datetime.now()
        
        try:
            # ë¬¸ì„œë³„ ë°ì´í„° ì¤€ë¹„
            documents_data = []
            
            for i, doc in enumerate(request.documents):
                doc_id = doc_ids[i]
                
                # ë©”íƒ€ë°ì´í„° ë¶„ë¦¬
                all_metadata = doc.metadata or {}
                milvus_metadata = filter_milvus_metadata(all_metadata)  # Milvus í•„í„°ë§ìš©
                
                # ì²­í¬ì— ì´ë¯¸ ì„ë² ë”©ì´ í¬í•¨ë˜ì–´ ìˆìŒ
                chunks_with_embeddings = []
                for chunk in doc.chunks:
                    chunks_with_embeddings.append({
                        "chunk_index": chunk.chunk_index,
                        "embedding": chunk.embedding,  # ì œê³µëœ ì„ë² ë”© ì‚¬ìš©
                        "text": chunk.text
                    })
                
                documents_data.append({
                    "chat_bot_id": doc.chat_bot_id,
                    "doc_id": doc_id,
                    "content_name": doc.content_name,  # content_name ì¶”ê°€
                    "chunks": chunks_with_embeddings,
                    "metadata": milvus_metadata  # í•„í„°ë§ìš© ë©”íƒ€ë°ì´í„°ë§Œ Milvusì—
                })
            
            # Milvus ë°°ì¹˜ ì‚½ì…
            await milvus_client.batch_insert_vectors_with_retry(
                account_name=request.account_name,
                documents_data=documents_data,
                metadata={},  # ê°œë³„ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ê°€ ìš°ì„ ë¨
                max_retries=3,
                backoff=2.0
            )
            
            milvus_time = (datetime.now() - milvus_start).total_seconds() * 1000
            logger.info(f"âœ… Milvus ë°°ì¹˜ ë²¡í„° ì‚½ì… ì™„ë£Œ")
            
        except Exception as milvus_error:
            # Milvus ì‹¤íŒ¨ ì‹œ PostgreSQL ë¡¤ë°± (ë³´ìƒ íŠ¸ëœì­ì…˜)
            logger.error(f"âŒ Milvus ë°°ì¹˜ ì‚½ì… ì‹¤íŒ¨, PostgreSQL ë¡¤ë°± ì‹œì‘: {str(milvus_error)}")
            for doc_id in doc_ids:
                try:
                    await postgres_client.delete_document(request.account_name, request.documents[0].chat_bot_id, doc_id)
                except:
                    pass  # ë¡¤ë°± ì‹¤íŒ¨ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì†
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Milvus ë°°ì¹˜ ì‚½ì… ì‹¤íŒ¨: {str(milvus_error)}"
            )
        
        # ========== Step 4: ğŸ”¥ ìë™ flush ë§ˆí‚¹ (ì´ë²¤íŠ¸ ê¸°ë°˜) ==========
        await auto_flusher.mark_for_flush(collection_name)
        logger.info(f"ğŸ”¥ Flush marked: {collection_name}")
        
        total_time = (datetime.now() - start_time).total_seconds() * 1000
        
        # ì„±ê³µ ê²°ê³¼ ìƒì„±
        results = []
        for i, doc in enumerate(request.documents):
            results.append({
                "doc_id": doc_ids[i],
                "title": doc.metadata.get('title', '(ì œëª© ì—†ìŒ)') if doc.metadata else '(ì œëª© ì—†ìŒ)',
                "total_chunks": len(doc.chunks),
                "success": True
            })
        
        logger.info(f"âœ… ë°°ì¹˜ ì‚½ì… ì™„ë£Œ (ì„ë² ë”© í¬í•¨, ë§ˆì´ê·¸ë ˆì´ì…˜)")
        logger.info(f"   - ì„±ê³µ: {len(doc_ids)}ê°œ ë¬¸ì„œ")
        logger.info(f"   - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ms")
        logger.info(f"   - ì„ë² ë”© ìƒì„± ì‹œê°„: 0ms (ìŠ¤í‚µë¨)")
        logger.info(f"   - ê²€ìƒ‰ ê°€ëŠ¥ ì‹œê°„: 0.5ì´ˆ ì´ë‚´")
        
        from app.models.document import BatchInsertResult
        return BatchInsertResponse(
            status="success",
            total_documents=total_docs,
            total_chunks=total_chunks,
            success_count=len(doc_ids),
            failure_count=0,
            inserted_documents=len(doc_ids),
            total_vectors=total_chunks,
            results=[BatchInsertResult(**r) for r in results],
            postgres_insert_time_ms=postgres_time,
            embedding_time_ms=embedding_time,  # 0ms
            milvus_insert_time_ms=milvus_time,
            total_time_ms=total_time
        )
        
    except HTTPException:
        # HTTPExceptionì€ ê·¸ëŒ€ë¡œ ì¬ë°œìƒ
        raise
    except Exception as e:
        # ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ì‹œ PostgreSQL ë¡¤ë°±
        if doc_ids:
            logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ, PostgreSQL ë¡¤ë°± ì‹œì‘: {str(e)}")
            for doc_id in doc_ids:
                try:
                    await postgres_client.delete_document(request.account_name, request.documents[0].chat_bot_id, doc_id)
                except Exception as rollback_error:
                    logger.error(f"âŒ PostgreSQL ë¡¤ë°± ì‹¤íŒ¨ (doc_id: {doc_id}): {str(rollback_error)}")
        
        logger.error(f"âŒ ë°°ì¹˜ ì‚½ì… ì‹¤íŒ¨ (ì„ë² ë”© í¬í•¨): {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Batch insert with embeddings failed: {str(e)}"
        )


@router.get("/document/{doc_id}", response_model=DocumentResponse)
async def get_document(
    doc_id: int,
    account_name: str = Query(..., description="ê³„ì •ëª…"),
    chat_bot_id: str = Query(..., description="ì±—ë´‡ ID (UUID)")
):
    """
    ë¬¸ì„œ ì¡°íšŒ (doc_id ê¸°ë°˜)
    
    í•´ë‹¹ ê³„ì • ë° ë´‡ì˜ íŒŒí‹°ì…˜ì—ì„œ ë¬¸ì„œì™€ ëª¨ë“  ì²­í¬ ì¡°íšŒ
    """
    try:
        logger.info(f"ë¬¸ì„œ ì¡°íšŒ ìš”ì²­ (account: {account_name}, bot: {chat_bot_id}): doc_id={doc_id}")
        
        # TODO: PostgreSQLì—ì„œ ë¬¸ì„œ ì¡°íšŒ (ìë™ìœ¼ë¡œ í•´ë‹¹ íŒŒí‹°ì…˜ë§Œ ìŠ¤ìº”)
        # document = await postgres_client.get_document(account_name, chat_bot_id, doc_id)
        # chunks = await postgres_client.get_chunks(account_name, chat_bot_id, doc_id)
        
        raise HTTPException(
            status_code=status.HTTP_501_NOT_IMPLEMENTED,
            detail="êµ¬í˜„ ì˜ˆì •"
        )
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to retrieve document: {str(e)}"
        )


@router.put("/document/{doc_id}", response_model=DocumentUpdateResponse)
async def update_document(doc_id: int, request: DocumentUpdateRequest):
    """
    ë¬¸ì„œ ì „ì²´ ì—…ë°ì´íŠ¸
    
    ê¸°ì¡´ ë¬¸ì„œì˜ ëª¨ë“  ì²­í¬ë¥¼ ì‚­ì œí•˜ê³  ìƒˆë¡œìš´ ë°ì´í„°ë¡œ êµì²´
    """
    try:
        logger.info(f"ë¬¸ì„œ ì—…ë°ì´íŠ¸ ìš”ì²­ (account: {request.account_name}, bot: {request.chat_bot_id}): doc_id={doc_id}")
        
        # TODO: íŠ¸ëœì­ì…˜ ì²˜ë¦¬
        # async with transaction():
        #     chunk_count = len(request.chunks)
        #     document_data = {
        #         "chat_bot_id": request.chat_bot_id,
        #         "metadata": request.metadata or {}  # ëª¨ë“  ë©”íƒ€ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥
        #     }
        #     await postgres_client.update_document(request.account_name, request.chat_bot_id, doc_id, document_data, chunk_count)  # â­ ì²­í¬ ìˆ˜ ì „ë‹¬
        #     deleted_count = await postgres_client.delete_chunks(request.account_name, request.chat_bot_id, doc_id)
        #     await postgres_client.insert_chunks(request.account_name, request.chat_bot_id, doc_id, [c.dict() for c in request.chunks])
        #     await milvus_client.delete_by_doc_id(doc_id)
        #     embeddings = await embedding_service.batch_embed([chunk.text for chunk in request.chunks])
        #     await milvus_client.insert(doc_id, embeddings)
        
        raise HTTPException(
            status_code=status.HTTP_501_NOT_IMPLEMENTED,
            detail="êµ¬í˜„ ì˜ˆì •"
        )
    except Exception as e:
        logger.error(f"ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to update document: {str(e)}"
        )


@router.post("/document/delete", response_model=DocumentDeleteResponse)
async def delete_document(request: DocumentDeleteRequest):
    """
    ë¬¸ì„œ ì‚­ì œ (ì—¬ëŸ¬ content_name ê¸°ì¤€) - Saga Pattern ì ìš©
    
    PostgreSQLê³¼ Milvusì—ì„œ ì—¬ëŸ¬ ë¬¸ì„œì™€ ëª¨ë“  ì²­í¬ë¥¼ ì¼ê´„ ì‚­ì œí•©ë‹ˆë‹¤.
    - content_name ë¦¬ìŠ¤íŠ¸ë¡œ ì—¬ëŸ¬ ë¬¸ì„œ ì‹ë³„
    - Milvus: content_nameê³¼ chat_bot_idë¡œ í•„í„°ë§í•˜ì—¬ ëª¨ë“  ë²¡í„° ì‚­ì œ (ë¨¼ì €)
    - PostgreSQL: í•´ë‹¹ íŒŒí‹°ì…˜ì—ì„œ ì—¬ëŸ¬ ë¬¸ì„œì™€ ëª¨ë“  ì²­í¬ ì‚­ì œ (ë‚˜ì¤‘ì—)
    - PostgreSQL ì‹¤íŒ¨ ì‹œ Milvus ë³µêµ¬ (ë³´ìƒ íŠ¸ëœì­ì…˜)
    - ìë™ flushë¡œ ì‹¤ì‹œê°„ ë°˜ì˜
    
    Saga Pattern ì¥ì :
    - ë°ì´í„° ì¼ê´€ì„± ë³´ì¥
    - ë¶€ë¶„ ì‹¤íŒ¨ ì‹œ ìë™ ë³µêµ¬
    - ì—¬ëŸ¬ ë¬¸ì„œ ì¼ê´„ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ í–¥ìƒ
    - POST ë©”ì„œë“œë¡œ ìš”ì²­ ë³¸ë¬¸ì— ì•ˆì „í•˜ê²Œ ë°ì´í„° ì „ë‹¬
    """
    try:
        start_time = datetime.now()
        collection_name = f"collection_{request.account_name}"
        
        logger.info(f"ğŸ—‘ï¸ ë¬¸ì„œ ì¼ê´„ ì‚­ì œ ì‹œì‘ (Saga Pattern)")
        logger.info(f"   - Account: {request.account_name}")
        logger.info(f"   - Bot ID: {request.chat_bot_id}")
        logger.info(f"   - Content Names: {len(request.content_name)}ê°œ")
        logger.info(f"   - Collection: {collection_name}")
        
        # ========== Step 0: ì¡´ì¬í•˜ëŠ” ë¬¸ì„œë§Œ í•„í„°ë§ ==========
        existing_content_names = await postgres_client.get_existing_content_names(
            request.account_name, request.chat_bot_id, request.content_name
        )
        
        if not existing_content_names:
            # ì¡´ì¬í•˜ëŠ” ë¬¸ì„œê°€ ì—†ìŒ
            logger.warning(f"âš ï¸ ì¡´ì¬í•˜ëŠ” ë¬¸ì„œê°€ ì—†ìŒ: {request.content_name}")
            return DocumentDeleteResponse(
                status="success",
                message=f"No documents found to delete from {len(request.content_name)} requested",
                total_requested=len(request.content_name),
                total_success=0,
                total_failed=len(request.content_name),
                successful_content_names=[],
                failed_content_names=request.content_name,
                deleted_documents=0,
                deleted_chunks=0,
                deleted_vectors=0,
                postgres_delete_time_ms=0.0,
                milvus_delete_time_ms=0.0,
                total_time_ms=(datetime.now() - start_time).total_seconds() * 1000
            )
        
        # ì„±ê³µ/ì‹¤íŒ¨í•œ ë¬¸ì„œ ì¶”ì 
        successful_content_names = []
        failed_content_names = []
        
        # ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œë“¤ì„ ì‹¤íŒ¨ ëª©ë¡ì— ì¶”ê°€
        if len(existing_content_names) < len(request.content_name):
            missing_docs = set(request.content_name) - set(existing_content_names)
            failed_content_names.extend(list(missing_docs))
            logger.info(f"ğŸ“‹ ì¡´ì¬í•˜ëŠ” ë¬¸ì„œ: {len(existing_content_names)}ê°œ / {len(request.content_name)}ê°œ")
            logger.info(f"   - ì¡´ì¬í•˜ëŠ” ë¬¸ì„œ: {existing_content_names}")
            logger.info(f"   - ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œ: {list(missing_docs)}")
        else:
            logger.info(f"ğŸ“‹ ëª¨ë“  ë¬¸ì„œ ì¡´ì¬: {len(existing_content_names)}ê°œ")
            logger.info(f"   - ì¡´ì¬í•˜ëŠ” ë¬¸ì„œ: {existing_content_names}")
        
        # ========== Step 1: Milvusì—ì„œ ë²¡í„° ì¼ê´„ ì‚­ì œ (ë¨¼ì €) ==========
        milvus_start = datetime.now()
        deleted_vectors = 0
        
        try:
            # ì¡´ì¬í•˜ëŠ” content_nameê³¼ chat_bot_idë¡œ ì¼ê´„ ì‚­ì œ
            deleted_vectors = await milvus_client.delete_by_content_names(
                collection_name, request.chat_bot_id, existing_content_names
            )
            
            milvus_time = (datetime.now() - milvus_start).total_seconds() * 1000
            logger.info(f"âœ… Milvus ì¼ê´„ ì‚­ì œ ì™„ë£Œ: {deleted_vectors}ê°œ ë²¡í„°, {milvus_time:.2f}ms")
            
        except Exception as milvus_error:
            logger.error(f"âŒ Milvus ì¼ê´„ ì‚­ì œ ì‹¤íŒ¨: {str(milvus_error)}")
            # Milvus ì‚­ì œ ì‹¤íŒ¨ ì‹œ ëª¨ë“  ì¡´ì¬í•˜ëŠ” ë¬¸ì„œë¥¼ ì‹¤íŒ¨ ëª©ë¡ì— ì¶”ê°€
            failed_content_names.extend(existing_content_names)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Milvus batch deletion failed: {str(milvus_error)}"
            )
        
        # ========== Step 2: PostgreSQLì—ì„œ ë¬¸ì„œ ì¼ê´„ ì‚­ì œ (ë‚˜ì¤‘ì—) ==========
        postgres_start = datetime.now()
        
        try:
            deleted_docs, deleted_chunks = await postgres_client.delete_documents_by_content_names(
                request.account_name, request.chat_bot_id, existing_content_names
            )
            postgres_time = (datetime.now() - postgres_start).total_seconds() * 1000
            
            logger.info(f"âœ… PostgreSQL ì¼ê´„ ì‚­ì œ ì™„ë£Œ: {deleted_docs}ê°œ ë¬¸ì„œ, {deleted_chunks}ê°œ ì²­í¬, {postgres_time:.2f}ms")
            
            # PostgreSQL ì‚­ì œ ì„±ê³µ ì‹œ ëª¨ë“  ì¡´ì¬í•˜ëŠ” ë¬¸ì„œë¥¼ ì„±ê³µ ëª©ë¡ì— ì¶”ê°€
            successful_content_names.extend(existing_content_names)
            
        except Exception as postgres_error:
            # PostgreSQL ì‚­ì œ ì‹¤íŒ¨ ì‹œ Milvus ë³µêµ¬ (ë³´ìƒ íŠ¸ëœì­ì…˜)
            logger.error(f"âŒ PostgreSQL ì¼ê´„ ì‚­ì œ ì‹¤íŒ¨, Milvus ë³µêµ¬ ì‹œì‘: {str(postgres_error)}")
            try:
                # TODO: Milvus ë³µêµ¬ ë¡œì§ êµ¬í˜„ (ì‚­ì œëœ ë²¡í„°ë¥¼ ë‹¤ì‹œ ì‚½ì…)
                logger.warning(f"âš ï¸ Milvus ë³µêµ¬ ë¡œì§ ë¯¸êµ¬í˜„ - ë°ì´í„° ì¼ê´€ì„± ë¬¸ì œ ê°€ëŠ¥ì„±")
            except Exception as recovery_error:
                logger.error(f"âŒ Milvus ë³µêµ¬ ì‹¤íŒ¨: {str(recovery_error)}")
            
            # PostgreSQL ì‚­ì œ ì‹¤íŒ¨ ì‹œ ëª¨ë“  ì¡´ì¬í•˜ëŠ” ë¬¸ì„œë¥¼ ì‹¤íŒ¨ ëª©ë¡ì— ì¶”ê°€
            failed_content_names.extend(existing_content_names)
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"PostgreSQL batch deletion failed: {str(postgres_error)}"
            )
        
        # ========== Step 3: ğŸ”¥ ìë™ flush ë§ˆí‚¹ (ì‚­ì œ ì´ë²¤íŠ¸) ==========
        await auto_flusher.mark_for_flush(collection_name)
        logger.info(f"ğŸ”¥ Flush marked after delete: {collection_name}")
        
        # ========== Step 4: ê²°ê³¼ ë°˜í™˜ ==========
        total_time = (datetime.now() - start_time).total_seconds() * 1000
        
        # ì‘ë‹µ ë©”ì‹œì§€ ìƒì„±
        if len(successful_content_names) == len(request.content_name):
            # ëª¨ë“  ë¬¸ì„œê°€ ì„±ê³µí•œ ê²½ìš°
            status = "success"
            message = f"All {len(request.content_name)} documents deleted successfully"
        elif len(successful_content_names) > 0:
            # ì¼ë¶€ ë¬¸ì„œë§Œ ì„±ê³µí•œ ê²½ìš°
            status = "partial_success"
            message = f"Deleted {len(successful_content_names)} out of {len(request.content_name)} requested documents"
        else:
            # ëª¨ë“  ë¬¸ì„œê°€ ì‹¤íŒ¨í•œ ê²½ìš°
            status = "failed"
            message = f"Failed to delete any of {len(request.content_name)} requested documents"
        
        logger.info(f"âœ… ë¬¸ì„œ ì¼ê´„ ì‚­ì œ ì™„ë£Œ (Saga Pattern ì„±ê³µ)")
        logger.info(f"   - ìš”ì²­ëœ ë¬¸ì„œ: {len(request.content_name)}ê°œ")
        logger.info(f"   - ì„±ê³µí•œ ë¬¸ì„œ: {len(successful_content_names)}ê°œ")
        logger.info(f"   - ì‹¤íŒ¨í•œ ë¬¸ì„œ: {len(failed_content_names)}ê°œ")
        logger.info(f"   - ì‚­ì œëœ ë¬¸ì„œ: {deleted_docs}ê°œ")
        logger.info(f"   - ì‚­ì œëœ ì²­í¬: {deleted_chunks}ê°œ")
        logger.info(f"   - ì‚­ì œëœ ë²¡í„°: {deleted_vectors}ê°œ")
        logger.info(f"   - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ms")
        logger.info(f"   - ê²€ìƒ‰ì—ì„œ ì œì™¸: 0.5ì´ˆ ì´ë‚´")
        
        return DocumentDeleteResponse(
            status=status,
            message=message,
            total_requested=len(request.content_name),
            total_success=len(successful_content_names),
            total_failed=len(failed_content_names),
            successful_content_names=successful_content_names,
            failed_content_names=failed_content_names,
            deleted_documents=deleted_docs,
            deleted_chunks=deleted_chunks,
            deleted_vectors=deleted_vectors,
            postgres_delete_time_ms=postgres_time,
            milvus_delete_time_ms=milvus_time,
            total_time_ms=total_time
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete document: {str(e)}"
        )


@router.post("/bot/delete", response_model=BotDeleteResponse, status_code=status.HTTP_200_OK)
async def delete_bot_data(request: BotDeleteRequest):
    """
    ë´‡ ì „ì²´ ë°ì´í„° ì‚­ì œ (chat_bot_id ê¸°ì¤€) - Saga Pattern ì ìš©
    
    í•´ë‹¹ ë´‡ì˜ ëª¨ë“  ë¬¸ì„œì™€ ì²­í¬ë¥¼ PostgreSQLê³¼ Milvusì—ì„œ ì‚­ì œí•©ë‹ˆë‹¤.
    - Milvus: í•´ë‹¹ íŒŒí‹°ì…˜ì˜ ëª¨ë“  ë²¡í„° ì‚­ì œ (ë¨¼ì €)
    - PostgreSQL: í•´ë‹¹ ë´‡ì˜ ëª¨ë“  ë¬¸ì„œì™€ ì²­í¬ ì‚­ì œ (ë‚˜ì¤‘ì—)
    - PostgreSQL ì‹¤íŒ¨ ì‹œ Milvus ë³µêµ¬ (ë³´ìƒ íŠ¸ëœì­ì…˜)
    - ìë™ flushë¡œ ì‹¤ì‹œê°„ ë°˜ì˜
    
    Saga Pattern ì¥ì :
    - ë°ì´í„° ì¼ê´€ì„± ë³´ì¥
    - ë¶€ë¶„ ì‹¤íŒ¨ ì‹œ ìë™ ë³µêµ¬
    - POST ë©”ì„œë“œë¡œ ìš”ì²­ ë³¸ë¬¸ì— ì•ˆì „í•˜ê²Œ ë°ì´í„° ì „ë‹¬
    """
    try:
        start_time = datetime.now()
        
        logger.info(f"ğŸ—‘ï¸ ë´‡ ì „ì²´ ì‚­ì œ ì‹œì‘ (Saga Pattern)")
        logger.info(f"   - Account: {request.account_name}")
        logger.info(f"   - Bot ID: {request.chat_bot_id}")
        
        collection_name = f"collection_{request.account_name}"
        partition_name = generate_partition_name(request.chat_bot_id)
        
        # ========== Step 1: Milvusì—ì„œ íŒŒí‹°ì…˜ ì‚­ì œ (ë¨¼ì €) ==========
        milvus_start = datetime.now()
        deleted_vectors = 0
        
        try:
            # Milvusì—ì„œ í•´ë‹¹ íŒŒí‹°ì…˜ì˜ ëª¨ë“  ë²¡í„° ì‚­ì œ
            deleted_vectors = await milvus_client.delete_partition(collection_name, partition_name)
            
            milvus_time = (datetime.now() - milvus_start).total_seconds() * 1000
            logger.info(f"âœ… Milvus íŒŒí‹°ì…˜ ì‚­ì œ ì™„ë£Œ: {deleted_vectors}ê°œ ë²¡í„°, {milvus_time:.2f}ms")
            
        except Exception as milvus_error:
            logger.error(f"âŒ Milvus íŒŒí‹°ì…˜ ì‚­ì œ ì‹¤íŒ¨: {str(milvus_error)}")
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Milvus partition deletion failed: {str(milvus_error)}"
            )
        
        # ========== Step 2: PostgreSQLì—ì„œ ë´‡ ë°ì´í„° ì‚­ì œ (ë‚˜ì¤‘ì—) ==========
        postgres_start = datetime.now()
        
        try:
            deleted_docs, deleted_chunks = await postgres_client.delete_bot_data(request.account_name, request.chat_bot_id)
            postgres_time = (datetime.now() - postgres_start).total_seconds() * 1000
            logger.info(f"âœ… PostgreSQL ë´‡ ì‚­ì œ ì™„ë£Œ: {deleted_docs}ê°œ ë¬¸ì„œ, {deleted_chunks}ê°œ ì²­í¬, {postgres_time:.2f}ms")
            
        except Exception as postgres_error:
            # PostgreSQL ì‚­ì œ ì‹¤íŒ¨ ì‹œ Milvus ë³µêµ¬ (ë³´ìƒ íŠ¸ëœì­ì…˜)
            logger.error(f"âŒ PostgreSQL ë´‡ ì‚­ì œ ì‹¤íŒ¨, Milvus ë³µêµ¬ ì‹œì‘: {str(postgres_error)}")
            try:
                # TODO: Milvus ë³µêµ¬ ë¡œì§ êµ¬í˜„ (ì‚­ì œëœ íŒŒí‹°ì…˜ì„ ë‹¤ì‹œ ìƒì„±í•˜ê³  ë²¡í„° ì¬ì‚½ì…)
                # í˜„ì¬ëŠ” ë¡œê·¸ë§Œ ë‚¨ê¸°ê³  ê³„ì† ì§„í–‰
                logger.warning(f"âš ï¸ Milvus ë³µêµ¬ ë¡œì§ ë¯¸êµ¬í˜„ - ë°ì´í„° ì¼ê´€ì„± ë¬¸ì œ ê°€ëŠ¥ì„±")
            except Exception as recovery_error:
                logger.error(f"âŒ Milvus ë³µêµ¬ ì‹¤íŒ¨: {str(recovery_error)}")
            
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"PostgreSQL bot deletion failed: {str(postgres_error)}"
            )
        
        # ========== Step 3: ğŸ”¥ ìë™ flush ë§ˆí‚¹ (ì‚­ì œ ì´ë²¤íŠ¸) ==========
        await auto_flusher.mark_for_flush(collection_name)
        logger.info(f"ğŸ”¥ Flush marked after bot delete: {collection_name}")
        
        # ========== Step 4: ê²°ê³¼ ë°˜í™˜ ==========
        total_time = (datetime.now() - start_time).total_seconds() * 1000
        
        logger.info(f"âœ… ë´‡ ì „ì²´ ì‚­ì œ ì™„ë£Œ (Saga Pattern ì„±ê³µ)")
        logger.info(f"   - Bot ID: {request.chat_bot_id}")
        logger.info(f"   - ì‚­ì œëœ ë¬¸ì„œ: {deleted_docs}ê°œ")
        logger.info(f"   - ì‚­ì œëœ ì²­í¬: {deleted_chunks}ê°œ")
        logger.info(f"   - ì‚­ì œëœ ë²¡í„°: {deleted_vectors}ê°œ")
        logger.info(f"   - ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ms")
        logger.info(f"   - ê²€ìƒ‰ì—ì„œ ì œì™¸: 0.5ì´ˆ ì´ë‚´")
        
        return BotDeleteResponse(
            status="success",
            chat_bot_id=request.chat_bot_id,
            deleted_documents=deleted_docs,
            deleted_chunks=deleted_chunks,
            deleted_vectors=deleted_vectors,
            postgres_delete_time_ms=postgres_time,
            milvus_delete_time_ms=milvus_time,
            total_time_ms=total_time
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ ë´‡ ì „ì²´ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete bot data: {str(e)}"
        )


@router.patch("/document/{doc_id}/metadata", response_model=MetadataUpdateResponse)
async def update_metadata(doc_id: int, request: MetadataUpdateRequest):
    """
    ë©”íƒ€ë°ì´í„°ë§Œ ì—…ë°ì´íŠ¸
    
    PostgreSQLë§Œ ìˆ˜ì •í•˜ê³  MilvusëŠ” ê±´ë“œë¦¬ì§€ ì•ŠìŒ (ì´ˆê³ ì†)
    """
    try:
        logger.info(f"ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ìš”ì²­ (account: {request.account_name}, bot: {request.chat_bot_id}): doc_id={doc_id}")
        
        # TODO: PostgreSQL UPDATEë§Œ ì‹¤í–‰ (í•´ë‹¹ íŒŒí‹°ì…˜ì—ì„œë§Œ)
        # await postgres_client.update_metadata(request.account_name, request.chat_bot_id, doc_id, request.metadata_updates)
        
        raise HTTPException(
            status_code=status.HTTP_501_NOT_IMPLEMENTED,
            detail="êµ¬í˜„ ì˜ˆì •"
        )
    except Exception as e:
        logger.error(f"ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to update metadata: {str(e)}"
        )





